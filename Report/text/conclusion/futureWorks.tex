\chapter{Future works}
\label{chap:further_works}
The results from the evaluation method show that this is a viable way of evaluating a programming language in a low-cost setup. However, there are still plenty of opportunities which can be taken into consideration for improving the method. This chapter provides a discussion of the most promising things that can be done in the future.

One of the things that could be done would be to conduct the usability experiment on Quorum still with experienced programmers using the Sodbeans environment.
This would give a more direct comparison of the differences in the data gotten from using the usability experiment method versus our method.

Of course simply conducting our method on more languages, and ideally by the language designers of these languages, would also give a lot of data about the method.
One way to facilitate this could be to spread the method to the 4th semester students in the engineering faculty (SW, DAT, IT) at Aalborg University.
Since these student have to design a language as part of their semester project anyway and could use the data to argument for their language design decisions, it would be an opportune way of testing the method in a low-risk environment.

While our method is good for finding problems in a programming language, it is less well suited to be used to compare the quality of programming languages.
It could be interesting to look at creating a method designed for that purpose, as it such comparisons commonly are of interest to language designers looking to promote their language over existing ones.
One way of creating such a method from our method, could be to create a set of generalised tasks that would be applicable on all programming languages, which would give a solid common ground for the comparison.
It might however be literally impossible to create such a tasks set, as programming languages can be quite varied and might not have any common ground.
Specialised languages tend to omit a lot of features of a general-purpose language, and even within more general purpose languages there can be huge differences (for example when switching programming paradigm) that would make a general task set impossible.
A more likely way to use our method would be to compare languages for a specific application area, as it is possible to create tasks that are common in that area and apply the same task to several languages for comparison.

Ideally, conducting empirical experiments relies on the use of a very diverse test group - participants with a different age, occupation and geological location in order to get a sufficient variance in the results. In our case, the participants were mostly in the same age group (20-25), had very similar occupation (4th to 8th Semester students in an engineering degree - CS, DAT, SW) and had a very similar geological location (Nordjylland, within the Aalborg area). This certainly leaves a room for improvement, where conducting the experiment with a more diverse test group might yield different results.

Given the qualitative nature of the IDA method and the minimum number of participants needed for a viable experiment, we did not need a significant amount of participants for drawing adequate results. However, it could be interesting to see how the evaluation method would fare in a quantitative setup, involving hundreds or thousands of participants. We leave this as something to be considered in the future since testing on such as group at this point is beyond the scope of this project.

Another thing worth exploring is using our method on novices.
Using experienced programmers makes it easier to convey how to program in a language, as they already know how to program, and it  makes sense when programmers are the target group for the language.
I does however mean that the data tend to be biased towards the languages the programmers already know.
Using novices avoids this bias and is obviously useful for languages designed for them.
It does however present a challenge for our method as novices are less familiar with the act of programming and are more prone to feel lost without the assistance of an IDE.
One example of this is participant \#2 who we have otherwise omitted from the results since he was not an experienced programmer.
In this test the participant was completely unable to write anything before the facilitator stepped in and essentially dictated exactly what should be written for the first subtask.
After this the participant was however capable of solving the second and third subtask on his own.
This shows that it is definitely not impossible to test on novices, but it probably requires some additional thought put into the task and sample sheet.
Conducting the experiment on novices would give us a better idea of what kind of alterations the experiment setup would need to facilitate those experiments.

Based on the previous point, a good thing to do would be to design and conduct the experiment with a pre-made skeleton set up for the tasks like we mentioned in \secref{Discussion:TaskSamples}.
This would give us a much better idea about how that affects the experiment, and would allow us to better gauge how well it prevents the freezing issue it was designed to combat.
If we were to conduct the experiment on novices, this would also be a likely addition to that test as we believe they are the most likely to need the additional guidance. 

%using a silent compiler in the experiment to allow the participant to get feedback, but without getting assistance from the tool.