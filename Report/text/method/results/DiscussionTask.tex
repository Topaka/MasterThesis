\section{Tasks \& Samples}
\label{Discussion:TaskSamples}
It is difficult to create good tasks, as we have experienced in our experiments.
Firstly we had to determine which parts of the language we wanted to test.
For our C\# test it was object-oriented programming, for our F\# test it was recursive functions and for our quorum test it was arithmetic operations, operations on containers, objects and classes and operations on strings.
Then we tried to build a scenario that would give us tasks that exercise that decision.
The specific scenario was not too important but having one helps convey the task, and gives some ideas for examples and names, which eases some unintended creative burden on the participant.
Next we had to decide how to formulate the tasks.
Here we ran into the problem of how strictly one wants the process to be.
For our first tests on C\# and F\# we intentionally kept the tasks vague to ensure we would not corrupt the data.
This however allowed the massive variance in task completion we had between our two participants.
It is possible this could have been avoided if we had more strictly defined tasks, however that would also have caused us to lose the data we got about the intuitiveness of using inheritance.
For our test on Quorum, we used more smaller tasks in several different scenarios instead of one big scenario as in the previous tests.
This meant we had to have a more strict formulation of the tasks.
Otherwise there was a higher risk of any tasks taking up all the time as it was interpreted to be larger or more complex than intended, which we discovered in our pilot test.
However even with a stricter formulation we ran into participants overcomplicating tasks by interpreting them in a larger scope of the scenario than we had intended.
At the same time we also encountered some participants not being able to start on the first task as they simply did not know where to start on a blank piece of paper.
One way to make the formulation even stricter without dictating what to write could be to provide the skeleton of the solution we expect.
This might also fix the issue with the blank page paralysis as it gives the participant a starting point to work from.
It does of course lose the data about how the participant would structure the solution as we would have done so for them.

Similar problems was found when trying to define a sample sheet.
Obviously we wanted to at least have examples demonstrating at least the minimum of functionality we expected to be necessary to solve our tasks.
However, only using the bare minimum limited the data we could gather, as the participant would be pushed into using one particular functionality where another might also have been used.
An example of this could be that in our sample sheet we only included the \textit{repeat while} loop, which means we would not be able to get any data about the \textit{repeat times} or \textit{repeat until} loops.
To avoid this problem it would be an option to include examples of more functionality than what is strictly needed to give the participants more freedom in choosing their constructs.
However, this would require a larger sample sheet, which would likely be another problem.
Our participants would often only skim through the sample sheet, and some directly expressed that it felt cumbersome to shuffle through even three pages to find the example they were looking for.
Increasing the size of the sample sheet would likely only exacerbate this problem.
To fix this problem, one of our participants suggested adding a "cheat sheet" to the samples, that would have all the functionality summarised on a single page.
Creating such a "cheat sheet" would however present a challenge of its own as it can be difficult to convey all the functionality on a single page.
Also a lot of our participant expressed appreciation at the use of working samples in our sample sheet, since it gave them a more complete image of how the code should look.
From this we believe it is more important to focus on the bigger sheet, but if you can create a "cheat sheet" it could be a worthwhile addition.
Another thing to consider when creating a task sheet is the order of your examples relative to the order of the tasks.
Some of our participants expected the samples in the sample sheet to follow the tasks in order of relevance.
This occasionally gave a few errors, like for example the problem where a participant used \lstinline!output! instead of \lstinline!return!, was a result of \lstinline!output! being demonstrated in the early examples while \lstinline!return! was not demonstrated until the late examples.
The participant had not seen the \lstinline!return! example before doing the first task, and upon seeing the \lstinline!output! keyword just assumed that was it.
Being more mindful of ordering of our examples could possible have avoided this problem, but there might be a risk of it biasing the participants.
Especially if one is going for demonstrating an excess of functionality, there is a risk that whichever alternative is shown first, would be the preferred alternative due to this bias.
One last thing to note is that in our sample sheet one of the examples is a solution to one of the subtasks.
While this could be an interesting way to test the participants attention to the sample sheet, it is probably best to avoid this in order to avoid biasing the solution to that task.