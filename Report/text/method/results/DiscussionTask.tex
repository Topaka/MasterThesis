\section{Tasks \& Samples}
It is difficult to create good tasks, as we have experienced in our experiments.
Firstly we had to determine which parts of the language we wanted to test.
For our C\# test it was object-oriented programming, for our F\# test it was recursive functions and for our quorum test it was arithmetic operations, operations on containers, objects and classes and operations on strings.
Then we tried to build a scenario that would give us tasks that exercise that decision.
The specific scenario was not too important but having one helps convey the task, and gives some ideas for examples and names, which eases some unintended create burden on the participant.
Next we had to decide what how to formulate the tasks.
Here we ran into the problem of how strictly one wants the process to go.
For our first tests on C\# and F\# we intentionally kept the tasks vague to ensure we would not corrupt the data.
This however allowed the massive variance in task completion we had between our two participants.
It is possible this could have been avoided if we had more strictly defined tasks, however that would also have caused us to lose the data we got about the intuitiveness of using inheritance.
For our test on Quorum, we used more smaller tasks in several different scenarios instead of one big scenario as in the previous tests.
This meant we had to have a more strict formulation of the tasks.
Otherwise there was a higher risk of any tasks taking up all the time as it was interpreted to be larger or more complex than intended, which we discovered in our pilot test.
However even with a stricter formulation we ran into participants overcomplicating tasks by interpreting them in a larger scope of the scenario than we had intended.
At the same time we also encountered some participants not being able to start on the first task as they simply did not know where to start on a blank piece of paper.
One way to make the formulation even stricter without dictating what to write could be to provide the skeleton of the solution we expect.
This might also fix the issue with the blank page paralysis as it gives the participant a starting point to work from.
It does of course lose the data about how the participant would structure the solution as we would have done so for them.


remember to use the output instead of return error