\chapter{Related Work}
\label{chap:related_work}

Programming languages have been used for many years, but there still has not been established a robust and efficient way to asses and evaluate them. However, plenty of research has been done on the topic and specific papers address that to a different degree as it will be shown in this chapter. 

M. Farooq et al. 2014 \cite{FPLEvalFramework} wrote a paper introducing an evaluation framework which provides a comparative analysis of widely used first programming languages (FPLs), or namely languages which are used as a first language for teaching introductory programming. The framework is based on technical and environmental features and it is used for testing the suitability of existing imperative and object oriented languages as appropriate FPLs. In order to support their framework, they have devised a customizable scoring function for computing a  suitability score for a given language which helps quantify and rank languages based on the given criterion. Lastly, they evaluated the most widely used FPLs by computing their suitability scores. The novelty in their work stems from the definition of the evaluation parameters as well as the related characteristics for evaluating each parameter.

K. Parker et al. 2006 \cite{LangSelectionProcess} note that the process of selecting a programming language for introductory courses is informal and it lacks structure and replicability. In order to address that, a more structured approach is proposed, which enables a more detailed evaluation of the selection process. The paper presents an exhaustive list of selection criteria, where each criterion is assigned weights in order to determine its relative importance in the selection process. This is tested by scoring different programming languages according to said criteria. The proposed approach is verified by an informal pilot study to assess the completeness of the criteria and gather enough feedback on the process itself. Given the dynamic nature of programming paradigms and languages, the authors acknowledge that the selection criteria and process can be revised.

Stefik \& Siebert \cite{QuorumRandomo} have conducted an empirical study comparing how accurately novices in programming can write programs by the use of three different programming languages - Quorum, Perl and Randomo. Quorum is an evidence-based programming language, developed by them with the use of survey methods and usability tests, Perl is an already established language and Randomo, as the name suggests, is a language with a randomly generated syntax and whose keywords and symbols were picked from the ASCII table, making it a Placebo language. The results from the empirical study showed that Quorum had significantly higher accuracy among the novice participants compared to Perl and Randomo, both of which had very similar accuracy rates. The authors contribute the higher percentage of Quorum to its very careful use of evidence while designing the language. However, they admit that the results might be skewed given the small test group, and repeatedly conducting the experiment might yield entirely different results.

The interest in visual programming languages has been steadily increasing for the past few decades as graphic support in both software and hardware becomes more and more prominent. J. Kiper et al. \cite{VisualLangsEval} 1997, present a method of evaluating visual programming languages by a set of evaluation criteria. The basis of their work is influenced by the taxonomy provided by Singh and Chignell \cite{VisualComputer} which divides visual computing in three key areas - \textit{programming computers, end-user interaction with computers and visualization}. This set of five criteria - visual nature, functionality, ease of comprehension, paradigm support and scalability try to constitute the philosophy of a "complete" visual programming language. Each criteria on its own is supported by a set of metrics. This evaluation method not only allows assessment of an individual visual programming language but also the comparison of elements of a set of languages.

Using HCI to assist the design of programming languages is not a new idea.
John F. Pane et al. \cite{HANDS} have used HCI principles in the design of their language called HANDS.
They used research from Psychology of Programming (PoP), Empirical Studies of Programmers (ESP) and related fields, summarised in "Usability Issues in the Design of Novice Programming Systems"\cite{UsabilityNoviceProgramming}, to guide their language design decisions.
Then they conducted a user study on HANDS and a version of HANDS with three features omitted (queries, aggregate operations, and the high visibility of data) to evaluate these three features.
The study was conducted on 23 fifth-grade children where, of the children who made it through the tutorial, 9 were in the HANDS group and 9 were in the limited HANDS group.
In this study, the children in the HANDS group performed significantly better, solving a total of 19 tasks across the group in contrast with the limited HANDS group's one solved task.

Raymond P.L. Buse et al. \cite{UserEvalPapers} conducted a study on 3110 papers in software engineering research in order to analyse statistics on their usage of user studies.
They found that, in general, the number of user studies in software engineering papers have been increasing in the last years.
Also, for prestigious conferences (ones with a low paper acceptance rate) the percentage of papers with user evaluations was higher.
They did not find a correlation between user evaluation and citation count outside the papers with a high citation count.
This is interesting for us since it shows an increasing interest in including user evaluation, which we are looking for a cheaper way of doing.
The paper also found that a user evaluation with 1-5 participants performed equal to or better than using more participants, in terms of average citation count.
This is particularly interesting for us since it shows that using a discount evaluation method over a heavier one does not impact the citation count (and thus the perceived impact) of a paper.

Daniel P. Delorey et al. \cite{LanguageProductivity} conducted a large scale user study, in a different way, by doing statistical analysis on 9,999 open source projects hosted on SourceForge.net.
The goal of their study was to examine whether programming languages have an effect on programmer productivity.
They found that programming languages do indeed affect productivity, which is a great motivator to improve programming languages.
However, they note that this study is insufficient to prove the correlation, partially because they use lines of code to measure programmer productivity, and this might not be an accurate metric.
This kind of study does, however, allow one to gather and analyse a very large dataset to prove one's claims, but it is impossible to do before large repositories of projects in a language have been written.
This is unlikely to be true, unless the language already has widespread use, so it is not a good way to measure performance of a new language.