\chapter{Existing evaluation methods}
\label{chap:eval_methods}

Programming language designers and researchers have been involved in the process of evaluating programming languages and their constructs since the very beginning, dating back to 1950s and 1960s, with the inclusion of languages such as Autocode, COBOL, Lisp \cite{PLHistory}. The process of coming up with a programming language on the concept level is rarely under a scientific rigour and it usually involves whatever means the designers have at their disposal. As for the actual implementation of a language, being a constitute of its individual parts such as lexer,parser, virtual machine etc., there are certain guidelines and established methods one can follow which does not usually leave a lot of room for deviation. However, things are different when programming languages are being evaluated since the community has different opinions of how this is done. Different programming languages have different design decisions and intended purposes while each adheres to a particular programming paradigm(s). This is turn means that the same criterion might have a varying weight if put as the basis of an evaluation of two distinct languages. Researchers have partially agreed on a list of evaluation criteria and several means of validation. R.W. Sebesta \cite{Sebesta} classifies the most important ones as \textit{Readibility, Writeability, Reliability} and \textit{Cost}, which are in turn influenced by different characteristics (e.g. Simplicity, Data types, Syntax design etc.) \todo{maybe something additional could be added here}

This chapter will describe some of these means of validation and how they can be applicable to our work.

\section{Performance benchmarks}

Parallel programming languages are mainly used in real life applications where speed and efficiency are key factors. In order to evaluate the effectiveness of such languages, performance benchmarks have been developed. S. Nanz et al. \cite{MulticoreLangs} describe an experiment where four different parallel programming languages (Chapel, Cilk, Go and Threading Building Blocks(TBB)) are used to implement sequential and parallel versions of six performance benchmarks, taking into account factors such as source code size, coding time, execution time and speedup. The benchmarks made use of a partial set of the Cowichan problems \cite{CowichanProblems} since "\textit{reusing a tried and tested set has the benefit that
estimates for the implementation complexity exist and that
problem selection bias can be avoided by the experimenter}." as stated by Nanz at el. \cite{MulticoreLangs}. Additionally, H.-W. Loidl et al, present in their work a detailed performance measurements on three fundamentally different parallel functional languages -  PMLS, a system for implicitly parallel execution of ML programs; GPH, a mainly implicit parallel extension of Haskell; and Eden, a more explicit parallel extension of Haskell designed for both distributed and parallel execution. All three systems are tested on a widely available parallel architecture - a Beowulf cluster of low-cost workstations. The benchmarks made use of three algorithms - a matrix multiplication algorithm, an exact linear system solver and a ray-tracer. \todo{maybe add something else here?}

\section{Usability Frameworks}

\section{Case \& User studies}

\section{Empirical experiments}