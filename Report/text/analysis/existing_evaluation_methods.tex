\chapter{Existing evaluation methods}
\label{chap:eval_methods}

Programming language designers and researchers have been involved in the process of evaluating programming languages and their constructs since the very beginning, dating back to 1950s and 1960s, with the inclusion of languages such as Autocode, COBOL, Lisp \cite{PLHistory}. The process of coming up with a programming language on the concept level is rarely under a scientific rigour and it usually involves whatever means the designers have at their disposal. As for the actual implementation of a language, being a constitute of its individual parts such as lexer,parser, virtual machine etc., there are certain guidelines and established methods one can follow which does not usually leave a lot of room for deviation. However, things are different when programming languages are being evaluated since the community has different opinions of how this is done. Different programming languages have different design decisions and intended purposes while each adheres to a particular programming paradigm(s). This is turn means that the same criterion might have a varying weight if put as the basis of an evaluation of two distinct languages. Researchers have partially agreed on a list of evaluation criteria and several means of validation. R.W. Sebesta \cite{Sebesta} classifies the most important ones as \textit{Readibility, Writeability, Reliability} and \textit{Cost}, which are in turn influenced by different characteristics (e.g. Simplicity, Data types, Syntax design etc.) \todo{maybe something additional could be added here}

This chapter will describe some of these means of validation and how they can be applicable to our work.

\section{Performance benchmarks}

Parallel programming languages are mainly used in real life applications where speed and efficiency are key factors. In order to evaluate the effectiveness of such languages, performance benchmarks have been developed. S. Nanz et al. \cite{MulticoreLangs} describe an experiment where four different parallel programming languages (Chapel, Cilk, Go and Threading Building Blocks(TBB)) are used to implement sequential and parallel versions of six performance benchmarks, taking into account factors such as source code size, coding time, execution time and speedup. The benchmarks made use of a partial set of the Cowichan problems \cite{CowichanProblems} since "\textit{reusing a tried and tested set has the benefit that
estimates for the implementation complexity exist and that
problem selection bias can be avoided by the experimenter}." as stated by Nanz at el. \cite{MulticoreLangs}. Additionally, H.-W. Loidl et al, present in their work a detailed performance measurements on three fundamentally different parallel functional languages -  PMLS, a system for implicitly parallel execution of ML programs; GPH, a mainly implicit parallel extension of Haskell; and Eden, a more explicit parallel extension of Haskell designed for both distributed and parallel execution. All three systems are tested on a widely available parallel architecture - a Beowulf cluster of low-cost workstations. The benchmarks made use of three algorithms - a matrix multiplication algorithm, an exact linear system solver and a ray-tracer. \todo{maybe add something else here?}

\section{Usability Frameworks}
Treating programming languages as if they were user interfaces is not something new and addressing their usability has been one of the topics studied in the field of Human-Computer Interaction (HCI) for quite some time. "The Psychology of Computer Programming" by Gerald Weinberg \cite{GWeinberg} is one of the earliest works on the psychological aspect of computer programming which dates back to 1971. Thomas Green and M. Petre \cite{CognitiveDimensions} came up with the Cognitive Dimensions framework under which programming languages are considered as interactive systems and are evaluated based on a set of dimensions such as \textit{closeness of mapping, consistency, hidden dependencies, viscosity} (which is resistance to local change) and others. The framework has been tested on Prolog by Allan Blackwell \cite{CognitiveDimensions2} and some visual programming languages \cite{InstructsDescripts},\cite{InformationArtefacts},\cite{CognitiveDimensions} as well as an early version of C\# by Steven Clarke \cite{NewLangEval}. John Pane and Brad Myers et al \cite{NaturalProgramming}, have worked on the evaluation of programming systems and identifying common usability and design issues of novice programming systems as a way to get a better understanding of how language designers can make use of HCI methods into their work. Their work culminated in a 10-year project called "Natural programming" which has the goal of bringing programming "closer to the way people think" which also resulted in new programming languages and environments. 
\\More often than not, modern implementations of programming languages are closely tied to a specific IDE (Integrated Development Environment) which makes it very hard to treat the language as a disjoint entity. When trying to evaluate a programming language, is it fair to evaluate the language itself or the language and the IDE? Farooq and Zirkler \cite{APPPeer} have proposed an evaluation method for application programming interfaces (APIs) which relies on peer reviews and it is mostly centered around the APIs rather than the programming languages themselves. Leonel Diaz \cite{LangsasUI} had a different approach to the problem by identifying two different dimensions of evaluation for programming languages and their environments -  The IDE-to-Program user interface and the Language-to-Machine user interface. The first one treats the IDE as a user interface to the program, not the language, which in turn allows to elaborate on how the interface can be evaluated. In the second dimension, the language is seen as the user interface to the hardware resources to the machine.		

\section{Case \& User studies}
Other approaches make use of case studies and user studies which might range from informal form to controlled empirical experiments. Such controlled experiments \cite{DynamicTypeSystems},\cite{MulticoreLangs},\cite{EmpiricalComparison}, \cite{Empiricalinvestigation} subject group(s) of human subjects to solve programming tasks with varying difficulty in different languages in controlled environments. The results from such controlled experiments provide valuable data on how programming languages could be evaluated as well as what impact certain features of a programming language might have such as syntax and typing. One drawback to that approach is that the variety and number of tasks solved could be limiting and the participant are often novices in programming. This is in contrast to real-world programming where code bases grow constantly and change over time over many development iterations. For this reason, empirical studies have been conducted on code repositories (e.g. Github \cite{GitHub}) analysing large quantities of code written by many experienced developers  over long periods of time. This provides valuable feedback on defect proneness and code evolution, but it is not as focused as programming assignments since there is a great disparity between projects and the categories they encompass. Nanz \cite{RosettaCode} conducted a comparative study of 8 programming languages in Rosetta Code, each representing a major programming paradigm, serving as a middle ground between small programming assignments and big software projects. A total of 7087 solutions have been analysed as a part of a quantitative statistical analysis corresponding to 745 different tasks taking into account programming features such as conciseness, running time, memory usage and error proneness.

\section{Quantitative experiments }

Another approach of conducting experiments in programming language research and evaluation is to rely on quantitative data rather than qualitative one. In such experiments a hypothesis is either proved or disproved, by executing a statistical analysis on the results, usually filtering out external factors in the process. The scope in such quantitative experiments could vary in the number of participants taking part - ranging from tens (comparative experiment on Quorum, Perl and Randomo by Stefik \& Siebert \cite{QuorumRandomo}) to tens of thousands (quantitative study on a very large data set from GitHub by Ray \& Posnett \cite{GitHubExperiment}). Additionally, the nature of the experiment could warrant to use of two different groups, tested under different conditions, where the results are analysed for significant differences between the two groups. Examples of such could be the completion of a set of programming tasks, each group using a different programming language to solve them or simply having the same exact programming setup for both groups, but having different prior knowledge. Stefik and Hanenberg have conducted few such empirical experiments \cite{StaticTypes},\cite{ProgLangSyntax} researching the impact of static and dynamic type systems as well as programming language syntax. Also, Stefik and Hanenberg address something called "the programming language wars" evident in the programming language community \cite{ProgrammingWars}. They elaborate on language \textit{divergence} (too many different programming languages) and what the \textit{impact} of each programming language on the community is. A common belief is that one programming language could solve the problem by being better than all the rest. The authors strongly disagree with this believe since essentially problem domains vary and as well the people involved which makes the creation of a single perfect language a myth. They believe that language designers duplicate effort by repeatedly trying designs which have already been tried by others since design decisions are rarely backed by scientific evidence. They stress the importance of designing languages based on evidence and teaching empirical methods to programming language students.