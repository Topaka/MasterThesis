\chapter{Existing evaluation methods}
\label{chap:eval_methods}

Programming language designers and researchers have been involved in the process of evaluating programming languages and their constructs since the very beginning, dating back to 1950s and 1960s, with the inclusion of languages such as Autocode, COBOL and Lisp \cite{PLHistory}. The process of coming up with a programming language on the concept level is rarely under scientific rigour, and it usually involves whatever means the designers have at their disposal. As for the actual implementation of a language, being a constitute of its individual parts (such as lexer, parser, virtual machine etc.), there are certain guidelines and established methods one can follow, which does not usually leave a lot of room for deviation. However, things are different when programming languages are being evaluated since the community has different opinions of how this is done. Different programming languages have different design decisions and intended purposes, while each adheres to one or more programming paradigms. This, in turn, means that the same criterion might have a varying weight if put as the basis of an evaluation of two distinct languages. Researchers have partially agreed on a list of evaluation criteria and several means of validation. R.W. Sebesta \cite{Sebesta} classifies the most important ones as \textit{Readibility, Writeability, Reliability} and \textit{Cost}, which are in turn influenced by different characteristics (e.g. Simplicity, Data types, Syntax design etc.)

This chapter will describe some of these means of validation, and how they can be applicable to our work.

\section{Performance benchmarks}

Parallel programming languages are mainly used in real life applications where speed and efficiency are key factors. In order to evaluate the effectiveness of such languages, performance benchmarks have been developed. S. Nanz et al. \cite{MulticoreLangs} describe an experiment where four different parallel programming languages (Chapel, Cilk, Go and Threading Building Blocks (TBB)) are used to implement sequential and parallel versions of six performance benchmarks, taking into account factors such as source code size, coding time, execution time and speedup. The benchmarks made use of a partial set of the Cowichan problems \cite{CowichanProblems} since "\textit{reusing a tried and tested set has the benefit that
estimates for the implementation complexity exist and that
problem selection bias can be avoided by the experimenter}." as stated by Nanz et al. \cite{MulticoreLangs}. Additionally, H.-W. Loidl et al. \cite{ParallelFunctLangs} present in their work a detailed performance measurements on three fundamentally different parallel functional languages: PMLS, a system for implicitly parallel execution of ML programs; GPH, a mainly implicit parallel extension of Haskell; and Eden, a more explicit parallel extension of Haskell designed for both distributed and parallel execution. All three systems are tested on a widely available parallel architecture - a Beowulf cluster of low-cost workstations. The benchmarks made use of three algorithms: a matrix multiplication algorithm, an exact linear system solver and a ray-tracer. 

Since we do not consider performance as a criteria we wish to evaluate on, we do not consider such benchmarks applicable to our work.

\section{Usability Frameworks}
Treating programming languages as if they were user interfaces is not something new, and addressing their usability has been one of the topics studied in the field of Human-Computer Interaction (HCI) for quite some time. "The Psychology of Computer Programming" by Gerald Weinberg \cite{GWeinberg} is one of the earliest works on the psychological aspect of computer programming, which dates back to 1971. Thomas Green and M. Petre \cite{CognitiveDimensions} came up with the Cognitive Dimensions framework, under which, programming languages are considered as interactive systems, and are evaluated based on a set of dimensions such as \textit{closeness of mapping, consistency, hidden dependencies, viscosity} (which is resistance to local change) and others. The framework has been tested on Prolog by Allan Blackwell \cite{CognitiveDimensions2}, some visual programming languages \cite{InstructsDescripts}\cite{InformationArtefacts}\cite{CognitiveDimensions} and an early version of C\# by Steven Clarke \cite{NewLangEval}. Brad Myers et al. \cite{NaturalProgramming} have worked on the evaluation of programming systems and identifying common usability and design issues of novice programming systems.
The goal of this was to, get a better understanding of how language designers can make use of HCI methods in their work. This culminated in a 10-year project called \textit{Natural programming}, which has the goal of bringing programming "closer to the way people think." This resulted in new programming languages and environments\cite{NaturalProgrammingEnvironment}. 

More often than not, modern implementations of programming languages are closely tied to a specific IDE (Integrated Development Environment) which makes it very hard to treat the language as a disjoint entity. When trying to evaluate a programming language, is it fair to evaluate the language itself or the language and the IDE? Farooq and Zirkler \cite{APPPeer} have proposed an evaluation method for application programming interfaces (APIs), which relies on peer reviews and is mostly centered around the APIs rather than the programming languages. Leonel Diaz \cite{LangsasUI} had a different approach to the problem by identifying two different dimensions of evaluation for programming languages and their environments: The IDE-to-Program user interface and the Language-to-Machine user interface. The first one treats the IDE as a user interface to the program, not the language, which in turn allows elaboration on how the interface can be evaluated. In the second dimension, the language is seen as the user interface to the hardware resources of the machine.

Usability frameworks fall into the discipline of HCI, therefore we consider this an area of interest for our work.		

\section{Case \& User studies}
Other approaches make use of case studies and user studies which might range from being informal to being controlled empirical experiments. Such controlled experiments \cite{DynamicTypeSystems}\cite{MulticoreLangs}\cite{EmpiricalComparison} \cite{Empiricalinvestigation} ask group(s) of human participants to solve programming tasks with varying difficulty in different languages in controlled environments. The results from such controlled experiments provide valuable data on how programming languages could be evaluated, as well as what impact certain features of a programming language, like syntax and typing, might have. One drawback to that approach is that the variety and number of tasks solved could be limiting. This is in contrast to real-world programming where code bases grow constantly and change over many development iterations. For this reason, empirical studies have been conducted on code repositories (e.g. Github \cite{GitHub}) analysing large quantities of code, written by many experienced developers over long periods of time. This provides valuable feedback on defect proneness and code evolution, but it is not as focused as programming assignments since there is a great disparity between projects and the categories they encompass. Nanz \cite{RosettaCode} conducted a comparative study of 8 programming languages in Rosetta Code, each representing a major programming paradigm, serving as a middle ground between small programming assignments and big software projects. A total of 7087 solutions have been analysed as a part of a quantitative statistical analysis corresponding to 745 different tasks, taking into account programming features such as conciseness, running time, memory usage and error proneness.

Case and user studies involve controlled empirical experiments. Therefore we consider them an area of interest for our project.

\section{Quantitative experiments}

Another approach of conducting experiments in programming language research and evaluation is to rely on quantitative data rather than qualitative data. In such experiments a hypothesis is either proved or disproved, by executing a statistical analysis on the results, usually filtering out external factors in the process. The scope in such quantitative experiments could vary in the number of participants taking part - ranging from tens (comparative experiment on Quorum, Perl and Randomo by Stefik \& Siebert \cite{QuorumRandomo}) to tens of thousands (quantitative study on a very large data set from GitHub by Ray \& Posnett \cite{GitHubExperiment}). Additionally, the nature of the experiment could warrant to use of two different groups, tested under different conditions, where the results are analysed for significant differences between the two groups. Examples of such could be the completion of a set of programming tasks, each group using a different programming language to solve them or simply having the same exact programming setup for both groups, but having different prior knowledge. Stefik and Hanenberg have conducted few such empirical experiments \cite{StaticTypes}\cite{ProgLangSyntax} researching the impact of static and dynamic type systems and programming language syntax. Also, Stefik and Hanenberg address something called "the programming language wars" evident in the programming language community \cite{ProgrammingWars}. They elaborate on language \textit{divergence} (too many different programming languages) and what the \textit{impact} of each programming language on the community is. A common belief is that one programming language could solve the problem by being better than all the rest. The authors strongly disagree with this belief since problem domains and the people involved vary, which makes the creation of a single perfect language a myth. They believe that language designers duplicate effort by repeatedly trying designs which have already been tried by others, since design decisions are rarely backed by scientific evidence. They stress the importance of designing languages based on evidence and teaching empirical methods to programming language students.

Given our limited access to participants and our goal of a method using few participants, we will not consider quantitative experiments in our work.

\section{Language-to-language comparisons}
Another evaluation approach, which differs to the ones described earlier, is the direct comparison between a set of programming languages. Although it does not have the same degree of scientific rigour compared to the other approaches, it can still pretty accurately identify the strong and the weak points inherent to a particular language. Plenty of research have been conducted on that topic, dating back to the early eighties and involving a plethora of languages (such as C, C++, Pascal, Fortran, Ada 95, C\# and Java). Feuer and Gehani \cite{CandPascal},  made a comparison in 1982 of C and Pascal and argued what would be a good basis for such comparison. This involved the design philosophies of the two languages, followed by a simple program, written in each, and comparison of the features present in the two languages. Ultimately, they wanted to test the suitability of each of the languages in different problem domains. Pascal was also included in a study along Ada 95, by Murtagh and Hamilton \cite{AdaandPascal}, for deciding on language for an introductory computer science class. The selection involved testing particular language features against course objectives, resulting in Ada 95 being selected as the appropriate successor to Pascal. An earlier version of Ada was tested against C++ by Tang \cite{AdaandCplusplus} in order to show contrasts and commonalities of the two languages. In later stages, direct comparisons have been made of Java against C and Fortran \cite{CandFortranJava}, as well as more notably C\# and Java \cite{JavaCsharp}. 